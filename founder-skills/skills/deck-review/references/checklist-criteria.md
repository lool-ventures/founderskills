# Deck Review Checklist Criteria

35 criteria across 7 categories. Each criterion has an ID, label, definition, and evidence basis (the best-practice principle it's grounded in).

## Category 1 — Narrative Flow (5 items)

### `purpose_clear`
**Label:** Company purpose is clear and specific
**Pass:** One declarative sentence that a smart investor can repeat after 10 seconds. Follows formats like "[Category] for [ICP] that delivers [quantified outcome]."
**Fail:** Purpose is vague, buzzwordy, requires multiple readings, or is missing.
**Warn:** Purpose exists but is two sentences or slightly unclear.
**Basis:** Sequoia "Writing a Business Plan" — define the company in a single declarative sentence.

### `headlines_carry_story`
**Label:** Slide headlines are conclusions, not topics
**Pass:** Headlines state conclusions ("Retention is 92% at 6 months") rather than topics ("Retention").
**Fail:** Most headlines are single-word topics or generic labels.
**Warn:** Mixed — some headlines carry conclusions, others are topics.
**Basis:** DocSend/Sequoia — investors skim headlines first; each headline should be a conclusion.

### `narrative_arc_present`
**Label:** Narrative follows Problem-Solution-Proof-Ask arc
**Pass:** Core arc follows Problem → Solution → Why now → Proof → Ask. Story feels inevitable.
**Fail:** No discernible narrative arc; slides feel like a data dump or random ordering.
**Warn:** Arc is present but has gaps or awkward transitions.
**Basis:** Sequoia, DocSend, YC — consistent core arc across all frameworks.

### `strongest_proof_early`
**Label:** Strongest proof appears by slide 4
**Pass:** Best traction/proof point appears within first 4 slides. Hooks the reader.
**Fail:** Strongest proof is buried after slide 6.
**Warn:** Some proof early but the strongest evidence is mid-deck.
**Basis:** DocSend data — 78% abandon before slide 6, but if reader reaches slide 4, 82% finish.

### `story_stands_alone`
**Label:** Deck tells story without narration
**Pass:** Share-deck version is self-explanatory without verbal accompaniment.
**Fail:** Slides are cryptic without the founder explaining them (live-deck problem in a share-deck context).
**Warn:** Mostly stands alone but 1-2 slides need context.
**Basis:** DocSend — share decks must stand alone; investors spend ~2:30 of real attention.

## Category 2 — Slide Content (8 items)

### `problem_quantified`
**Label:** Problem slide quantifies pain
**Pass:** Problem slide identifies the user/buyer, the moment of pain, and quantifies it (time, money, risk, regulatory exposure, missed revenue). Uses concrete examples or customer quotes.
**Fail:** Problem is abstract, industry-level, or unquantified.
**Warn:** Problem is identified but quantification is weak or generic.
**Basis:** Sequoia framework — identify user/buyer, moment of pain, quantify cost of inaction.

### `solution_shows_workflow`
**Label:** Solution shows before→after, not feature list
**Pass:** Shows the "before → after" workflow change. Product screenshot or 3-step flow when possible. Focuses on value delivered.
**Fail:** Solution is a feature list or technical architecture diagram.
**Warn:** Some workflow shown but leans toward features.
**Basis:** Sequoia, DocSend — show workflow change, not feature list; focus on value, not architecture.

### `why_now_has_catalyst`
**Label:** Why-now has genuine macro catalyst
**Pass:** Cites a genuine catalyst: regulatory shift, new platform primitive, behavioral change, cost curve collapse. Answers "Why wasn't this built before?"
**Fail:** "Why now" is missing or says "AI is hot" / generic market timing.
**Warn:** Catalyst is present but weak or not clearly differentiated from competitors' timing.
**Basis:** Sequoia — "Why wasn't this built before?" Strongest at pre-seed/seed.

### `market_bottom_up`
**Label:** Market sizing uses bottom-up approach
**Pass:** Shows bottom-up sizing: ICP count × willingness to pay × adoption rate. Expandability as second layer.
**Fail:** Only shows inflated top-down TAM chart with no justification.
**Warn:** Has top-down with some bottom-up elements, but bottom-up not primary.
**Basis:** Sequoia, multiple sources — bottom-up sizing beats generic TAM charts.

### `competition_honest`
**Label:** Competition section is honest and substantive
**Pass:** Maps direct competitors + incumbents + "do nothing" / internal build. Uses 2-3 concrete differentiators (speed, cost, distribution, data advantage, regulatory fit, switching costs).
**Fail:** "No competition" or missing competition slide.
**Warn:** Competition acknowledged but differentiators are generic or 2x2 matrix without explanation.
**Basis:** DocSend — VCs spent 88% more time on competition in successful decks vs. unsuccessful.

### `business_model_clear`
**Label:** Business model explains money flow and margins
**Pass:** Pricing model clear (seat, usage, outcome, per workflow, take-rate). Who pays vs. who uses distinguished. Gross margin structure shown (especially for AI/compute-heavy products).
**Fail:** No business model slide or just "SaaS" with no detail.
**Warn:** Pricing mentioned but gross margin or buyer vs. user distinction missing.
**Basis:** Sequoia framework — pricing model, buyer vs. user, gross margin structure.

### `gtm_has_proof`
**Label:** GTM slide has ICP, channel, and early proof
**Pass:** ICP + channel (PLG, sales-led, partner-led) defined. Sales cycle + ACV expectations. Early channel proof (pipeline, conversion, CAC, payback).
**Fail:** No GTM slide or just "we'll do marketing."
**Warn:** ICP and channel mentioned but no conversion data or pipeline proof.
**Basis:** Sequoia, DocSend — ICP + channel + early proof required.

### `team_has_depth`
**Label:** Team slide demonstrates founder-market fit
**Pass:** Bios, photos, LinkedIn links. Founder-market fit highlighted. Complementary skills shown. Advisory board if adding real credibility.
**Fail:** Just names or roles without context. No photos.
**Warn:** Some bios but founder-market fit not explicitly addressed.
**Basis:** DocSend — team slide gets ~46 seconds out of ~2:30 total (~30% of attention). Storydoc: 43% of reading time on interactive decks.

## Category 3 — Stage Fit (5 items)

### `stage_appropriate_structure`
**Label:** Slide order matches stage-specific framework
**Pass:** Slide ordering follows the reference outline for the detected stage.
**Fail:** Uses wrong stage's structure (e.g., pre-seed deck with Series A cohort data expectations).
**Warn:** Mostly appropriate but 1-2 slides out of expected order.
**Basis:** All sources — pre-seed, seed, and Series A have different emphasis and ordering.

### `stage_appropriate_traction`
**Label:** Traction metrics match stage expectations
**Pass:** Pre-seed: LOIs, waitlist, pilot data. Seed: ARR/MRR, growth, retention. Series A: cohorts, NRR, unit economics, burn multiple.
**Fail:** Wrong metrics for stage or no traction evidence at all.
**Warn:** Some appropriate metrics but missing key ones for stage.
**Basis:** Stage-specific guidelines — each stage has different proof requirements.

### `stage_appropriate_financials`
**Label:** Financial projections match stage depth
**Pass:** Pre-seed: 12-18 month budget + burn + runway + hiring plan. Seed: Operating model + revenue assumptions + gross margin + runway. Series A: Multi-year forecast + revenue by segment + headcount plan + path to profitability.
**Fail:** No financials or wildly inappropriate depth.
**Warn:** Financials present but missing stage-specific elements.
**Basis:** a16z, Carta — financial projection standards by stage.

### `ask_ties_to_milestones`
**Label:** Ask ties dollars to milestones to next round
**Pass:** Explicit fundraising ask + use of funds tied to milestones + next round readiness (or default-alive path). Addresses what happens if next round takes longer.
**Fail:** Ask is just "we need $X to grow" with no milestone plan.
**Warn:** Milestones mentioned but not clearly tied to dollars or next financing.
**Basis:** 2026 standard — tie dollars → milestones → next round readiness (or default-alive path).

### `round_size_realistic`
**Label:** Fundraising amount aligns with current benchmarks
**Pass:** Round size within 2025/2026 benchmarks: pre-seed $250K-$2.5M, seed $2M-$6M, Series A $10M-$20M.
**Fail:** Round size dramatically out of range for stage (e.g., $20M pre-seed).
**Warn:** On the edge of typical range but not unreasonable.
**Basis:** PitchBook/NVCA + Carta 2025 round-size data.

## Category 4 — Design & Readability (5 items)

### `one_idea_per_slide`
**Label:** One idea per slide
**Pass:** Each slide communicates a single concept clearly.
**Fail:** Multiple slides try to cover 2-3 distinct ideas each.
**Warn:** Mostly single-idea but 1-2 slides are overloaded.
**Basis:** All design guides — one idea per slide, no exceptions.

### `minimal_text`
**Label:** Big type, minimal paragraphs
**Pass:** 24pt+ body text minimum. Charts over text for traction. No dense paragraphs.
**Fail:** Small text, dense paragraphs, text-heavy slides throughout.
**Warn:** Mostly good but some slides have too much text.
**Basis:** DocSend, Storydoc — design for ~2:30 of real attention; big type, charts over text.

### `slide_count_appropriate`
**Label:** Core deck is 10-12 slides
**Pass:** 10-12 core slides (not counting appendix).
**Fail:** Fewer than 7 or more than 18 core slides.
**Warn:** 8-9 or 13-15 slides — slightly outside ideal range.
**Basis:** Storydoc data — ~10-slide decks achieve highest completion rate (32% vs. 22% average); sharp drop-off after ~18 slides.

### `consistent_design`
**Label:** Consistent visual design language
**Pass:** 2-3 primary colors, 1-2 fonts, generous white space. Professional and consistent throughout.
**Fail:** Inconsistent fonts, colors, spacing. Looks cobbled together.
**Warn:** Mostly consistent but some slides break the pattern.
**Basis:** Design rules — consistent design language with 2-3 primary colors, 1-2 fonts, generous white space.

### `mobile_readable`
**Label:** Readable on mobile without zoom
**Pass:** Big fonts, minimal text, charts readable without zoom. Passes "phone test."
**Fail:** Requires zooming on mobile to read text or charts. Tiny fonts.
**Warn:** Mostly readable but some slides have small elements.
**Basis:** Storydoc — 32% of decks opened on mobile (up to 70% for initial scans). Enforce "phone test."

## Category 5 — Common Mistakes (5 items)

### `no_vague_purpose`
**Label:** No vague or buzzwordy purpose statement
**Pass:** Purpose is specific, measurable, and immediately understandable.
**Fail:** Purpose uses buzzwords ("leveraging synergies," "paradigm shift," "revolutionary platform").
**Warn:** Purpose is clear but could be more specific.
**Basis:** Common mistakes — "One declarative sentence + measurable outcome."

### `no_nice_to_have_problem`
**Label:** Problem shows urgency, not a nice-to-have
**Pass:** Problem has urgency, a budget owner, and clear cost of inaction.
**Fail:** Problem is a "nice to have" with no urgency or budget owner.
**Warn:** Some urgency shown but cost of inaction not quantified.
**Basis:** Common mistakes — show urgency, budget owner, and cost of inaction.

### `no_hype_without_proof`
**Label:** No hype without supporting evidence
**Pass:** All claims backed by data, customer quotes, or verifiable evidence.
**Fail:** "Uber for X" positioning, inflated TAM, hockey stick projections without explanation.
**Warn:** Mostly substantiated but 1-2 claims need better support.
**Basis:** Common mistakes — hype without proof reliably kills momentum.

### `no_features_over_outcomes`
**Label:** Focuses on outcomes, not features
**Pass:** Focuses on value delivered and user outcomes, not technical architecture.
**Fail:** Primarily a feature list or technical spec.
**Warn:** Mix of outcomes and features; could lean more toward outcomes.
**Basis:** Common mistakes — features instead of outcomes.

### `no_dodged_competition`
**Label:** Competition slide exists and is substantive
**Pass:** Competition addressed honestly with alternatives and concrete differentiators.
**Fail:** No competition slide, or "we have no competitors."
**Warn:** Competition slide exists but is superficial.
**Basis:** DocSend — investors scrutinize this section more than founders think.

## Category 6 — AI Company (4 items)

Mark all as `not_applicable` if the company is not AI-first.

### `ai_retention_rebased`
**Label:** AI retention measured from Month 3
**Pass:** Retention analysis rebases from Month 0 to Month 3 to filter out "AI tourists."
**Fail:** Only shows Month 0 retention (inflated by novelty users).
**Warn:** Retention shown but not explicitly rebased.
**Basis:** a16z — analyze retention rebased from Month 3 for AI products.

### `ai_cost_to_serve_shown`
**Label:** Compute economics and margin trajectory shown
**Pass:** COGS breakdown (inference, data, hosting, support). Shows what improves margins (caching, model optimization, routing, pricing). Path to 60%+ gross margins.
**Fail:** No mention of compute costs or gross margin trajectory.
**Warn:** Some cost awareness but no clear margin improvement path.
**Basis:** a16z — failure to show path to 60%+ gross margins signals unscalable model.

### `ai_defensibility_beyond_model`
**Label:** Defensibility beyond "we use [foundation model]"
**Pass:** Shows at least one of: proprietary data / feedback loops, workflow distribution / embeddedness, regulatory/compliance moat, vertical specialization, agentic capabilities delivering transformational ROI.
**Fail:** Defensibility is just "we use GPT-4" or similar.
**Warn:** Some defensibility noted but not deeply substantiated.
**Basis:** a16z, multiple sources — API wrapper without moat is not defensible.

### `ai_responsible_controls`
**Label:** Responsible AI / risk controls addressed
**Pass:** Addresses privacy & data handling, eval & monitoring, guardrails/safety, compliance roadmap (SOC2, HIPAA, etc.). One slide — not overdone.
**Fail:** No mention of responsible AI practices for an enterprise-facing AI product.
**Warn:** Brief mention but missing key elements.
**Basis:** a16z — reduces deal friction, especially for enterprise.

## Category 7 — Diligence Readiness (3 items)

### `numbers_consistent`
**Label:** Claims in deck are internally consistent
**Pass:** Revenue, customer count, growth rate, market size, and financial projections are all mutually consistent.
**Fail:** Numbers contradict each other across slides (e.g., different ARR on traction vs. financials slide).
**Warn:** Minor inconsistencies that could be rounding or presentation differences.
**Basis:** a16z — deck must be consistent with what you'll show in the data room.

### `data_room_ready`
**Label:** Diligence materials referenced or available
**Pass:** Cap table + historical P&L + burn referenced or implied available. "We have a data room ready" or equivalent.
**Fail:** No mention of supporting materials; deck seems to exist in isolation.
**Warn:** Some materials implied but data room readiness not explicit.
**Basis:** a16z data room guide — cap table, historical P&L, burn are baseline diligence items.

### `contact_info_present`
**Label:** Contact information visible and correct
**Pass:** Email, LinkedIn, or other contact information clearly visible (typically on cover or closing slide).
**Fail:** No contact information anywhere in the deck.
**Warn:** Contact info present but hard to find.
**Basis:** Pre-send checklist — contact information visible and correct.
